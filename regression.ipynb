{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "\n",
    "# Suppress convergence warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.linear_model._coordinate_descent\")\n",
    "\n",
    "# Define configurations for models and tasks\n",
    "model_config = {\n",
    "    'bloom': {\n",
    "        'sizes': ['560M', '1b1', '1b7', '3b', '7b1'],\n",
    "        'n_parameters' : [560, 1100, 1700, 3000, 7100],\n",
    "        'jaccard_matrix': 'distances/jaccard_similarity/jaccard_matrix_bloom.csv',\n",
    "        'train_data_col': 'Bloom Train Data Percentage'\n",
    "    },\n",
    "    'bloomz': {\n",
    "        'sizes': ['560M', '1b1', '1b7', '3b', '7b1'],\n",
    "        'n_parameters' : [560, 1100, 1700, 3000, 7100],\n",
    "        'jaccard_matrix': 'distances/jaccard_similarity/jaccard_matrix_bloom.csv',\n",
    "        'train_data_col': 'Bloom Train Data Percentage',\n",
    "        'finetune_col': 'BLOOMZ Finetune Data'\n",
    "    },\n",
    "    'xglm': {\n",
    "        'sizes': ['564M', '1.7B', '2.9B', '7.5B'],\n",
    "        'n_parameters' : [564, 1700, 2900, 7500],\n",
    "        'jaccard_matrix': 'distances/jaccard_similarity/jaccard_matrix_xglm.csv',\n",
    "        'train_data_col': 'XGLM Train Percentage'\n",
    "    }\n",
    "}\n",
    "\n",
    "task_config = {\n",
    "    'zero_shot_classification': 'F1 {}-{}',\n",
    "    'two_shot_classification': 'F1 {}-{} 2s',\n",
    "    'zero_shot_generation': '{}-{} scbleu',\n",
    "    'two_shot_generation': '{}-{} scbleu 2s'\n",
    "}\n",
    "\n",
    "# Set current model and task\n",
    "for current_model in model_config.keys():\n",
    "    for current_task in task_config.keys():\n",
    "        # Load the language similarity matrix for the current model\n",
    "        jaccard_matrix_path = model_config[current_model]['jaccard_matrix']\n",
    "        jaccard_matrix_full = pd.read_csv(jaccard_matrix_path, header=None)\n",
    "        jaccard_matrix = jaccard_matrix_full.iloc[1:, 1:].astype(float).values\n",
    "\n",
    "        # Load the data\n",
    "        file_path = 'SIB-200 languages.xlsx'\n",
    "        data = pd.read_excel(file_path, sheet_name='Sheet1')\n",
    "        \n",
    "        # Standardize folder names (remove extra spaces and convert to lowercase)\n",
    "        data['Folder Name'] = data['Folder Name'].str.strip().str.lower()\n",
    "        jaccard_folder_names = jaccard_matrix_full.iloc[0, 1:].str.strip().str.lower().tolist()\n",
    "        \n",
    "        # Ensure the matching process works correctly\n",
    "        matching_indices = [jaccard_folder_names.index(folder) for folder in data['Folder Name'] if folder in jaccard_folder_names]\n",
    "        if not matching_indices:\n",
    "            raise ValueError(\"No matching folders found between the Jaccard matrix and the main dataset.\")\n",
    "        \n",
    "        # Filter the Jaccard matrix to include only matching folders\n",
    "        filtered_jaccard_matrix = jaccard_matrix[np.ix_(matching_indices, matching_indices)]\n",
    "        \n",
    "        # Handle NaN values in 'countries'\n",
    "        data['countries'] = data['countries'].apply(lambda x: eval(x) if pd.notna(x) else [])\n",
    "        \n",
    "        # Create country similarity matrix\n",
    "        def create_country_similarity(data):\n",
    "            country_list = data['countries'].tolist()\n",
    "            country_similarity = np.zeros((len(country_list), len(country_list)))\n",
    "            for i, countries_i in enumerate(country_list):\n",
    "                for j, countries_j in enumerate(country_list):\n",
    "                    if i != j:\n",
    "                        common_countries = set(countries_i).intersection(set(countries_j))\n",
    "                        total_countries = set(countries_i).union(set(countries_j))\n",
    "                        if len(total_countries) > 0:\n",
    "                            country_similarity[i, j] = len(common_countries) / len(total_countries)\n",
    "                        else:\n",
    "                            country_similarity[i, j] = 0\n",
    "            return country_similarity\n",
    "        \n",
    "        country_similarity_matrix = create_country_similarity(data)\n",
    "        \n",
    "        # MDS on country similarity matrix\n",
    "        mds_country = MDS(n_components=10, dissimilarity='precomputed', normalized_stress=False)\n",
    "        country_mds = mds_country.fit_transform(1 - country_similarity_matrix)\n",
    "        country_mds_df = pd.DataFrame(country_mds, columns=[f'Country_MDS{i+1}' for i in range(country_mds.shape[1])])\n",
    "        \n",
    "        # Handle geographical coordinates\n",
    "        geo_df = data[['latitude', 'longitude']].dropna()\n",
    "        coords = geo_df[['latitude', 'longitude']].values\n",
    "        \n",
    "        # Calculate geographical distance matrix\n",
    "        geo_distance_matrix = np.zeros((len(coords), len(coords)))\n",
    "        \n",
    "        for i, coord1 in enumerate(coords):\n",
    "            for j, coord2 in enumerate(coords):\n",
    "                geo_distance_matrix[i, j] = geodesic(coord1, coord2).kilometers\n",
    "        \n",
    "        # MDS on geographical distance matrix\n",
    "        mds_geo = MDS(n_components=10, dissimilarity='precomputed', normalized_stress=False)\n",
    "        geo_mds = mds_geo.fit_transform(geo_distance_matrix)\n",
    "        geo_mds_df = pd.DataFrame(geo_mds, columns=[f'Geo_MDS{i+1}' for i in range(geo_mds.shape[1])])\n",
    "        \n",
    "        # MDS on filtered Jaccard similarity matrix\n",
    "        mds_jaccard = MDS(n_components=10, dissimilarity='precomputed', normalized_stress=False)\n",
    "        jaccard_mds = mds_jaccard.fit_transform(1 - filtered_jaccard_matrix)\n",
    "        jaccard_mds_df = pd.DataFrame(jaccard_mds, columns=[f'Jaccard_MDS{i+1}' for i in range(jaccard_mds.shape[1])])\n",
    "        \n",
    "        # Merge the dataframes\n",
    "        data = data.merge(geo_mds_df, left_index=True, right_index=True, how='left')\n",
    "        data = data.merge(country_mds_df, left_index=True, right_index=True, how='left')\n",
    "        data = data.merge(jaccard_mds_df, left_index=True, right_index=True, how='left', suffixes=('', '_jaccard'))\n",
    "        \n",
    "        # Handle ordinal and categorical data\n",
    "        data['Population'] = data['Population'].replace({\n",
    "            '10K to 1 million': 0,\n",
    "            '1 million to 1 billion': 1,\n",
    "            '1 billion plus': 2,\n",
    "            'None': -1\n",
    "        }).fillna(-1)\n",
    "        \n",
    "        data['Language Vitality'] = data['Language Vitality'].replace({\n",
    "            'Extinct': 0,\n",
    "            'Endangered': 1,\n",
    "            'Stable': 2,\n",
    "            'Institutional': 3,\n",
    "            'None': -1\n",
    "        }).fillna(-1)\n",
    "        \n",
    "        data['Digital Language Support'] = data['Digital Language Support'].replace({\n",
    "            'Still': 0,\n",
    "            'Emerging': 1,\n",
    "            'Ascending': 2,\n",
    "            'Vital': 3,\n",
    "            'Thriving': 4,\n",
    "            'None': -1\n",
    "        }).fillna(-1)\n",
    "        \n",
    "        data['Resource Level'] = data['Resource Level'].replace('None', 0).fillna(0)\n",
    "        \n",
    "        # Convert categorical data\n",
    "        categorical_features = ['Script (ISO 15924)', 'Language Family']\n",
    "        data = pd.get_dummies(data, columns=categorical_features, dummy_na=True)\n",
    "        \n",
    "        # Handle missing Bloom Train Data Percentage or XGLM Train Percentage\n",
    "        train_data_col = model_config[current_model]['train_data_col']\n",
    "        data[train_data_col] = data[train_data_col].fillna(0)\n",
    "        \n",
    "        # Add BLOOMZ Finetune Data if applicable\n",
    "        if current_model == 'bloom':\n",
    "            data['BLOOMZ Finetune Data'] = data['BLOOMZ Finetune Data'].fillna(0)\n",
    "        \n",
    "        # List of model sizes for the current model\n",
    "        model_sizes = model_config[current_model]['sizes']\n",
    "        numeric_model_sizes = model_config[current_model]['n_parameters']\n",
    "        \n",
    "        # Prepare a combined DataFrame\n",
    "        combined_data = pd.DataFrame()\n",
    "        \n",
    "        for model_name, size in zip(model_sizes, numeric_model_sizes):\n",
    "            temp_data = data.copy()\n",
    "            temp_data['Model_Size'] = size\n",
    "            task_col = task_config[current_task].format(current_model, model_name)\n",
    "            temp_data['Performance'] = temp_data[task_col]\n",
    "            combined_data = pd.concat([combined_data, temp_data], ignore_index=True)\n",
    "        \n",
    "        # Define features and ensure there are no NaN values\n",
    "        features = (\n",
    "                [f'Geo_MDS{i+1}' for i in range(10)] +\n",
    "                [f'Country_MDS{i+1}' for i in range(10)] +\n",
    "                [f'Jaccard_MDS{i+1}' for i in range(10)] +\n",
    "                ['Population', 'Language Vitality', 'Digital Language Support', 'Resource Level', train_data_col, 'Model_Size'] +\n",
    "                list(combined_data.columns[combined_data.columns.str.startswith('Script (ISO 15924)_')]) +\n",
    "                list(combined_data.columns[combined_data.columns.str.startswith('Language Family_')])\n",
    "        )\n",
    "        \n",
    "        if current_model == 'bloomz':\n",
    "            features.append('BLOOMZ Finetune Data')\n",
    "        \n",
    "        # Ensure there are no NaN values in the feature matrix\n",
    "        combined_data = combined_data.dropna(subset=features + ['Performance'])\n",
    "        combined_data.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        X = combined_data[features]\n",
    "        y = combined_data['Performance']\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Define models to test\n",
    "        models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "            'SVR': SVR(),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "            'XGBoost': XGBRegressor(random_state=42),\n",
    "            'K-Nearest Neighbors': KNeighborsRegressor(),\n",
    "            'Lasso Regression': Lasso(random_state=42),\n",
    "            'Ridge Regression': Ridge(random_state=42),\n",
    "            'Elastic Net': ElasticNet(random_state=42)\n",
    "        }\n",
    "        \n",
    "        # Initialize results list\n",
    "        results_list = []\n",
    "        \n",
    "        # Train and evaluate each model\n",
    "        for model_name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            results_list.append({'Model': model_name, 'R2 Score': r2, 'MSE': mse})\n",
    "        \n",
    "        # Convert results list to DataFrame\n",
    "        results = pd.DataFrame(results_list)\n",
    "        results.to_csv(f'results/regression/{current_model}_{current_task}_results.csv', index=False)\n",
    "        # Display the results\n",
    "        results"
   ],
   "id": "e33f029fd5f81469",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "        import shap\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "        from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from sklearn.svm import SVR\n",
    "        from xgboost import XGBRegressor\n",
    "        from sklearn.neighbors import KNeighborsRegressor\n",
    "        from sklearn.inspection import permutation_importance\n",
    "        import os\n",
    "        \n",
    "        # Create results directory if it doesn't exist\n",
    "        results_dir = 'results/features'\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        # Function to train the selected regression model and compute feature importances\n",
    "        def train_and_compute_importances(model_name, model, X_train, y_train, X_test, y_test):\n",
    "            model.fit(X_train, y_train)\n",
    "        \n",
    "            if model_name in ['XGBoost', 'Random Forest', 'Gradient Boosting', 'Decision Tree']:\n",
    "                feature_importances = model.feature_importances_\n",
    "                feature_names = X_train.columns\n",
    "                importance_values = feature_importances\n",
    "            else:\n",
    "                result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "                feature_names = X_train.columns\n",
    "                importance_values = result.importances_mean\n",
    "        \n",
    "            # Create a DataFrame for better visualization\n",
    "            importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance_values})\n",
    "            importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "            return importance_df\n",
    "        \n",
    "        # Function to aggregate importances by abstract feature\n",
    "        def aggregate_importances(importance_df, abstract_features):\n",
    "            abstract_importances = {}\n",
    "        \n",
    "            for abstract_feature, sub_features in abstract_features.items():\n",
    "                total_importance = importance_df[importance_df['Feature'].isin(sub_features)]['Importance'].sum()\n",
    "                abstract_importances[abstract_feature] = total_importance\n",
    "        \n",
    "            abstract_importance_df = pd.DataFrame.from_dict(abstract_importances, orient='index', columns=['Importance'])\n",
    "            abstract_importance_df = abstract_importance_df.sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "            return abstract_importance_df\n",
    "        \n",
    "        # Select the model\n",
    "        models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "            'SVR': SVR(),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "            'XGBoost': XGBRegressor(random_state=42),\n",
    "            'K-Nearest Neighbors': KNeighborsRegressor(),\n",
    "            'Lasso Regression': Lasso(random_state=42),\n",
    "            'Ridge Regression': Ridge(random_state=42),\n",
    "            'Elastic Net': ElasticNet(random_state=42)\n",
    "        }\n",
    "        \n",
    "        for selected_model_name in ['XGBoost', 'Random Forest', 'Gradient Boosting']:  # Change this to select a different model\n",
    "            selected_model = models[selected_model_name]\n",
    "        \n",
    "            # Train the model and compute feature importances\n",
    "            importance_df = train_and_compute_importances(selected_model_name, selected_model, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "            # Define abstract features\n",
    "            abstract_features = {\n",
    "                'Geographical Features': [f'Geo_MDS{i+1}' for i in range(10)],\n",
    "                'Country Similarity': [f'Country_MDS{i+1}' for i in range(10)],\n",
    "                'Token Similarity': [f'Jaccard_MDS{i+1}' for i in range(10)],\n",
    "                'Script': [col for col in X_train.columns if col.startswith('Script (ISO 15924)_')],\n",
    "                'Language Family': [col for col in X_train.columns if col.startswith('Language Family_')],\n",
    "                'Population': ['Population'],\n",
    "                'Language Vitality': ['Language Vitality'],\n",
    "                'Digital Language Support': ['Digital Language Support'],\n",
    "                'Resource Level': ['Resource Level'],\n",
    "                'Pre-train Data Percentage': [model_config[current_model]['train_data_col']],\n",
    "                'Model Size': ['Model_Size']\n",
    "            }\n",
    "        \n",
    "            if current_model == 'bloomz':\n",
    "                abstract_features['Instruction Tune Data'] = ['BLOOMZ Finetune Data']\n",
    "        \n",
    "            # Aggregate importances by abstract feature\n",
    "            abstract_importance_df = aggregate_importances(importance_df, abstract_features)\n",
    "            abstract_importance_df= abstract_importance_df.reset_index().sort_values(by='Importance', ascending=False).rename(columns={'index': 'Abstract Feature'}).reset_index(drop=True)\n",
    "            abstract_importance_df.to_csv(os.path.join(results_dir, f'importance/table/{current_model}_{current_task }_{selected_model_name}_abstract_feature_importances.csv'), index=False)\n",
    "\n",
    "            # Plot the abstract feature importances and save the plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(abstract_importance_df['Abstract Feature'], abstract_importance_df['Importance'])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.ylabel('Abstract Feature')\n",
    "            plt.title(f'Abstract Feature Importances based on {selected_model_name}')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.savefig(os.path.join(results_dir, f'importance/plot/{current_model}_{current_task }_{selected_model_name}_abstract_feature_importances.jpg'))\n",
    "            plt.close()\n",
    "        \n",
    "            # SHAP value calculation for the selected model\n",
    "            def compute_shap_values(model_name, model, X):\n",
    "                if model_name in ['XGBoost', 'Gradient Boosting', 'Random Forest', 'Decision Tree']:\n",
    "                    explainer = shap.Explainer(model, X)\n",
    "                else:\n",
    "                    explainer = shap.KernelExplainer(model.predict, X)\n",
    "        \n",
    "                shap_values = explainer(X, check_additivity=False)\n",
    "                return shap_values\n",
    "        \n",
    "            # Aggregate SHAP values by abstract feature\n",
    "            def aggregate_shap_and_feature_values(shap_values, feature_values, feature_names, abstract_features):\n",
    "                aggregated_shap_values = np.zeros((shap_values.shape[0], len(abstract_features)))\n",
    "                aggregated_feature_values = np.zeros((feature_values.shape[0], len(abstract_features)))\n",
    "                abstract_feature_names = []\n",
    "        \n",
    "                for i, (abstract_feature, sub_features) in enumerate(abstract_features.items()):\n",
    "                    feature_indices = [feature_names.index(sub_feature) for sub_feature in sub_features if sub_feature in feature_names]\n",
    "                    aggregated_shap_values[:, i] = shap_values[:, feature_indices].sum(axis=1)\n",
    "                    aggregated_feature_values[:, i] = feature_values[:, feature_indices].sum(axis=1)\n",
    "                    abstract_feature_names.append(abstract_feature)\n",
    "        \n",
    "                return aggregated_shap_values, aggregated_feature_values, abstract_feature_names\n",
    "        \n",
    "            # Compute SHAP values for the selected model\n",
    "            shap_values = compute_shap_values(selected_model_name, selected_model, X)\n",
    "        \n",
    "            # Aggregate SHAP values and feature values\n",
    "            aggregated_shap_values, aggregated_feature_values, abstract_feature_names = aggregate_shap_and_feature_values(\n",
    "                shap_values.values, X.values, X.columns.tolist(), abstract_features\n",
    "            )\n",
    "        \n",
    "            # Convert to SHAP values object for visualization\n",
    "            aggregated_shap_values_explanation = shap.Explanation(\n",
    "                values=aggregated_shap_values,\n",
    "                base_values=shap_values.base_values,\n",
    "                data=aggregated_feature_values,\n",
    "                feature_names=abstract_feature_names\n",
    "            )\n",
    "\n",
    "            shap_df = pd.DataFrame({'Abstract Feature': abstract_feature_names, 'SHAP value': np.abs(aggregated_shap_values).mean(0)}).sort_values(by='SHAP value', ascending=False).reset_index(drop=True)\n",
    "            shap_df.to_csv(os.path.join(results_dir, f'shap/table/{current_model}_{current_task }_{selected_model_name}_shap_values.csv'), index=False)\n",
    "\n",
    "            # Save the summary plot for abstract features\n",
    "            shap.summary_plot(aggregated_shap_values, feature_names=abstract_feature_names, plot_type='bar', show=False)\n",
    "            plt.savefig(os.path.join(results_dir, f'shap/plot/{current_model}_{current_task }_{selected_model_name}_shap_summary_plot_bar.jpg'))\n",
    "            plt.close()\n",
    "        \n",
    "            shap.summary_plot(aggregated_shap_values, features=aggregated_shap_values, feature_names=abstract_feature_names, show=False)\n",
    "            plt.savefig(os.path.join(results_dir, f'shap/plot/{current_model}_{current_task }_{selected_model_name}_shap_summary_plot.jpg'))\n",
    "            plt.close()\n",
    "        print(f'{current_model}_{current_task} done')"
   ],
   "id": "11ab842f2bc25633",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_excel('results/SHAP_results.xlsx', sheet_name='Zero-Classification')\n",
    "\n",
    "# Sorting the DataFrame by feature names in descending order\n",
    "df_sorted = df.sort_values(by='feature', ascending=False)\n",
    "\n",
    "# Plotting the bar charts in horizontal subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# Bloom\n",
    "axes[0].barh(df_sorted['feature'], df_sorted['Bloom'], color='b')\n",
    "axes[0].set_title('Bloom')\n",
    "\n",
    "# Bloomz\n",
    "axes[1].barh(df_sorted['feature'], df_sorted['Bloomz'], color='g')\n",
    "axes[1].set_title('Bloomz')\n",
    "\n",
    "# XGLM\n",
    "axes[2].barh(df_sorted['feature'], df_sorted['XGLM'], color='r')\n",
    "axes[2].set_title('XGLM')\n",
    "\n",
    "# Set common ylabel\n",
    "fig.text(0.04, 0.5, 'Features', va='center', rotation='vertical')\n",
    "# Set common xlabel\n",
    "fig.text(0.5, 0.04, 'SHAP Values', ha='center')\n",
    "\n",
    "plt.tight_layout(rect=[0.05, 0.05, 1, 1])\n",
    "plt.show()"
   ],
   "id": "46abb0974893db79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_excel('results/SHAP_results.xlsx', sheet_name='Two-Classification')\n",
    "\n",
    "# Sorting the DataFrame by feature names in descending order\n",
    "df_sorted = df.sort_values(by='feature', ascending=False)\n",
    "\n",
    "# Plotting the bar charts in horizontal subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "# Bloom\n",
    "axes[0].barh(df_sorted['feature'], df_sorted['Bloom'], color='b')\n",
    "axes[0].set_title('Bloom')\n",
    "axes[0].tick_params(axis='y', labelsize=12)  # Adjust y-tick label size\n",
    "\n",
    "# Bloomz\n",
    "axes[1].barh(df_sorted['feature'], df_sorted['Bloomz'], color='g')\n",
    "axes[1].set_title('Bloomz')\n",
    "axes[1].tick_params(axis='y', labelsize=12)  # Adjust y-tick label size\n",
    "\n",
    "# XGLM\n",
    "axes[2].barh(df_sorted['feature'], df_sorted['XGLM'], color='r')\n",
    "axes[2].set_title('XGLM')\n",
    "axes[2].tick_params(axis='y', labelsize=12)  # Adjust y-tick label size\n",
    "\n",
    "# Set common ylabel\n",
    "fig.text(0.04, 0.5, 'Features', va='center', rotation='vertical', fontsize=12)\n",
    "# Set common xlabel\n",
    "fig.text(0.5, 0.04, 'SHAP Values', ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0.05, 0.05, 1, 1])\n",
    "plt.show()"
   ],
   "id": "f03cf21816bac25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to create a plot for a given sheet\n",
    "def plot_shap_values(sheet_name, row_idx, axes):\n",
    "    df = pd.read_excel('results/SHAP_results.xlsx', sheet_name=sheet_name)\n",
    "\n",
    "    # Renaming the features\n",
    "    rename_dict = {\n",
    "        \"Instruction Tune Data\": \"Instruction Tuning Data\",\n",
    "        \"Geographical Features\": \"Geographical Proximity\"\n",
    "    }\n",
    "    df['feature'] = df['feature'].replace(rename_dict)\n",
    "\n",
    "    # Sorting the DataFrame by feature names in descending order\n",
    "    df_sorted = df.sort_values(by='feature', ascending=False)\n",
    "\n",
    "    # Define the title suffix based on the sheet name\n",
    "    if 'Zero' in sheet_name:\n",
    "        title_suffix = 'Zero-shot'\n",
    "    elif 'Two' in sheet_name:\n",
    "        title_suffix = 'Two-shot'\n",
    "\n",
    "    if 'Classification' in sheet_name:\n",
    "        title_suffix += ' Classification'\n",
    "    elif 'Generation' in sheet_name:\n",
    "        title_suffix += ' Generation'\n",
    "\n",
    "    # Plotting each model in the corresponding column\n",
    "    axes[row_idx, 0].barh(df_sorted['feature'], df_sorted['Bloom'], color='b')\n",
    "    axes[row_idx, 0].set_title(f'Bloom - {title_suffix}')\n",
    "    axes[row_idx, 0].tick_params(axis='y', labelsize=12)\n",
    "\n",
    "    axes[row_idx, 1].barh(df_sorted['feature'], df_sorted['Bloomz'], color='g')\n",
    "    axes[row_idx, 1].set_title(f'Bloomz - {title_suffix}')\n",
    "    axes[row_idx, 1].tick_params(axis='y', labelsize=12)\n",
    "\n",
    "    axes[row_idx, 2].barh(df_sorted['feature'], df_sorted['XGLM'], color='r')\n",
    "    axes[row_idx, 2].set_title(f'XGLM - {title_suffix}')\n",
    "    axes[row_idx, 2].tick_params(axis='y', labelsize=12)\n",
    "\n",
    "# Classification sheets\n",
    "classification_sheets = ['Zero-Classification', 'Two-Classification']\n",
    "\n",
    "# Generation sheets\n",
    "generation_sheets = ['Zero-Generation', 'Two-Generation']\n",
    "\n",
    "# Plotting the bar charts for classification in a grid of subplots\n",
    "fig1, axes1 = plt.subplots(nrows=len(classification_sheets), ncols=3, figsize=(18, 10), sharey=True)\n",
    "\n",
    "# Plot each classification sheet in the corresponding row\n",
    "for idx, sheet in enumerate(classification_sheets):\n",
    "    plot_shap_values(sheet, idx, axes1)\n",
    "\n",
    "# Set common ylabel for classification\n",
    "fig1.text(0.04, 0.5, 'Features', va='center', rotation='vertical', fontsize=16)\n",
    "# Set common xlabel for classification\n",
    "fig1.text(0.5, 0.04, 'SHAP Values', ha='center', fontsize=16)\n",
    "\n",
    "plt.tight_layout(rect=[0.05, 0.05, 1, 1])\n",
    "plt.show()\n",
    "\n",
    "# Plotting the bar charts for generation in a grid of subplots\n",
    "fig2, axes2 = plt.subplots(nrows=len(generation_sheets), ncols=3, figsize=(18, 10), sharey=True)\n",
    "\n",
    "# Plot each generation sheet in the corresponding row\n",
    "for idx, sheet in enumerate(generation_sheets):\n",
    "    plot_shap_values(sheet, idx, axes2)\n",
    "\n",
    "# Set common ylabel for generation\n",
    "fig2.text(0.04, 0.5, 'Features', va='center', rotation='vertical', fontsize=16)\n",
    "# Set common xlabel for generation\n",
    "fig2.text(0.5, 0.04, 'SHAP Values', ha='center', fontsize=16)\n",
    "\n",
    "plt.tight_layout(rect=[0.05, 0.05, 1, 1])\n",
    "plt.show()"
   ],
   "id": "c639af37c11b14bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to create an enhanced plot for zero-shot and two-shot on the same axes with a split for model and language features\n",
    "def plot_combined_shap_values(sheet_zero, sheet_two, axes, col_idx):\n",
    "    # Read both zero-shot and two-shot data\n",
    "    df_zero = pd.read_excel('results/SHAP_results.xlsx', sheet_name=sheet_zero)\n",
    "    df_two = pd.read_excel('results/SHAP_results.xlsx', sheet_name=sheet_two)\n",
    "\n",
    "    # Renaming the features and the model name in the DataFrame\n",
    "    rename_dict = {\n",
    "        \"Instruction Tune Data\": \"Instruction Tuning Data\",\n",
    "        \"Geographical Features\": \"Geographical Proximity\",\n",
    "        \"Script\": \"Script Type\",\n",
    "        \"Bloomz\": \"BloomZ\"  # Rename the model column\n",
    "    }\n",
    "    df_zero['feature'] = df_zero['feature'].replace(rename_dict)\n",
    "    df_two['feature'] = df_two['feature'].replace(rename_dict)\n",
    "\n",
    "    # Rename the columns if necessary\n",
    "    df_zero.rename(columns=rename_dict, inplace=True)\n",
    "    df_two.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "    # Define model features and language features\n",
    "    model_features = [\"Model Size\", \"Pre-train Data Percentage\", \"Instruction Tuning Data\"]\n",
    "    language_features = [\"Country Similarity\", \"Digital Language Support\", \"Geographical Proximity\",\n",
    "                         \"Language Family\", \"Script Type\", \"Language Vitality\", \"Population\", \"Resource Level\", \"Token Similarity\"]\n",
    "\n",
    "    # Separate the data into model and language features\n",
    "    df_zero_model = df_zero[df_zero['feature'].isin(model_features)].sort_values(by='feature', ascending=False)\n",
    "    df_zero_language = df_zero[df_zero['feature'].isin(language_features)].sort_values(by='feature', ascending=False)\n",
    "    df_two_model = df_two[df_two['feature'].isin(model_features)].sort_values(by='feature', ascending=False)\n",
    "    df_two_language = df_two[df_two['feature'].isin(language_features)].sort_values(by='feature', ascending=False)\n",
    "\n",
    "    # Concatenate the two parts to create a split effect\n",
    "    df_zero_sorted = pd.concat([df_zero_model, df_zero_language])\n",
    "    df_two_sorted = pd.concat([df_two_model, df_two_language])\n",
    "\n",
    "    # Define custom vibrant colors for zero-shot and two-shot with added transparency\n",
    "    colors_zero_shot = ['#FF6F61', '#6B5B95', '#88B04B']\n",
    "    colors_two_shot = ['#FFB3A7', '#B39EB5', '#C3D7A4']\n",
    "\n",
    "    # Plotting both zero-shot and two-shot results\n",
    "    bar_width = 0.35\n",
    "    indices = np.arange(len(df_zero_sorted['feature']))\n",
    "\n",
    "    # Plot the bars with added shadow and edge color\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.barh(indices + bar_width, df_zero_sorted.iloc[:, i + 1], height=bar_width, color=colors_zero_shot[i], edgecolor='black', linewidth=0.5, label='Zero-shot', alpha=0.9)\n",
    "        ax.barh(indices, df_two_sorted.iloc[:, i + 1], height=bar_width, color=colors_two_shot[i], edgecolor='black', linewidth=0.5, label='Two-shot', alpha=0.9)\n",
    "        ax.set_yticks(indices + bar_width / 2)\n",
    "        ax.set_yticklabels(df_zero_sorted['feature'], fontsize=12, fontweight='bold')\n",
    "        ax.set_title(['Bloom', 'BloomZ', 'XGLM'][i], fontsize=14, fontweight='bold', color='#333333')\n",
    "        ax.legend(loc='upper right', fontsize=10, shadow=True)\n",
    "        ax.grid(True, which='major', axis='x', linestyle='--', alpha=0.6)\n",
    "        ax.set_facecolor('#f9f9f9')\n",
    "\n",
    "    # Add a dashed horizontal line to split the model and language features\n",
    "    split_index = len(df_zero_model)\n",
    "    for ax in axes:\n",
    "        ax.axhline(y=split_index - bar_width, color='#333333', linewidth=1.5, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Enhance the plot aesthetics\n",
    "    for ax in axes:\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_color('#aaaaaa')\n",
    "        ax.tick_params(axis='x', colors='#333333')\n",
    "        ax.tick_params(axis='y', colors='#333333')\n",
    "\n",
    "# Classification sheets\n",
    "classification_sheets = ['Zero-Classification', 'Two-Classification']\n",
    "\n",
    "# Generation sheets\n",
    "generation_sheets = ['Zero-Generation', 'Two-Generation']\n",
    "\n",
    "# Plotting the bar charts for classification in a separate figure\n",
    "fig1, axes1 = plt.subplots(nrows=1, ncols=3, figsize=(15, 8), sharey=True)\n",
    "\n",
    "# Plot each classification model\n",
    "plot_combined_shap_values('Zero-Classification', 'Two-Classification', axes1, 0)\n",
    "\n",
    "# Set common y-label and x-label for classification\n",
    "fig1.text(0.04, 0.5, 'Features', va='center', rotation='vertical', fontsize=16, fontweight='bold', color='#333333')\n",
    "fig1.text(0.5, 0.04, 'SHAP Values', ha='center', fontsize=16, fontweight='bold', color='#333333')\n",
    "\n",
    "# Save the classification figure as a high-quality image\n",
    "plt.tight_layout(rect=[0.05, 0.05, 1, 1])\n",
    "fig1.savefig('figures/classification.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "# Plotting the bar charts for generation in a separate figure\n",
    "fig2, axes2 = plt.subplots(nrows=1, ncols=3, figsize=(15, 8), sharey=True)\n",
    "\n",
    "# Plot each generation model\n",
    "plot_combined_shap_values('Zero-Generation', 'Two-Generation', axes2, 0)\n",
    "\n",
    "# Set common y-label and x-label for generation\n",
    "fig2.text(0.04, 0.5, 'Features', va='center', rotation='vertical', fontsize=16, fontweight='bold', color='#333333')\n",
    "fig2.text(0.5, 0.04, 'SHAP Values', ha='center', fontsize=16, fontweight='bold', color='#333333')\n",
    "\n",
    "# Save the generation figure as a high-quality image\n",
    "plt.tight_layout(rect=[0.05, 0.05, 1, 1])\n",
    "fig2.savefig('figures/generation.png', dpi=600, bbox_inches='tight')"
   ],
   "id": "3b883cfed35ddae8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize the first prediction's explanation with abstract features\n",
    "shap.plots.waterfall(shap_values[712])"
   ],
   "id": "27a8103ace78d4bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pylab as pl\n",
    "import numpy as np\n",
    "\n",
    "# Compute SHAP interaction values for the selected model\n",
    "shap_interaction_values = shap.TreeExplainer(selected_model).shap_interaction_values(X)\n",
    "\n",
    "# Aggregate SHAP interaction values and feature values by abstract features\n",
    "def aggregate_shap_interaction_values(shap_interaction_values, feature_values, feature_names, abstract_features):\n",
    "    n_samples = shap_interaction_values.shape[0]\n",
    "    n_abstract_features = len(abstract_features)\n",
    "\n",
    "    aggregated_shap_interaction_values = np.zeros((n_samples, n_abstract_features, n_abstract_features))\n",
    "    aggregated_feature_values = np.zeros((n_samples, n_abstract_features))\n",
    "\n",
    "    abstract_feature_names = list(abstract_features.keys())\n",
    "\n",
    "    for i, (abstract_feature_i, sub_features_i) in enumerate(abstract_features.items()):\n",
    "        for j, (abstract_feature_j, sub_features_j) in enumerate(abstract_features.items()):\n",
    "            feature_indices_i = [feature_names.index(sub_feature) for sub_feature in sub_features_i if sub_feature in feature_names]\n",
    "            feature_indices_j = [feature_names.index(sub_feature) for sub_feature in sub_features_j if sub_feature in feature_names]\n",
    "\n",
    "            aggregated_shap_interaction_values[:, i, j] = shap_interaction_values[:, feature_indices_i][:, :, feature_indices_j].sum(axis=(1, 2))\n",
    "\n",
    "        aggregated_feature_values[:, i] = feature_values[:, feature_indices_i].mean(axis=1)\n",
    "\n",
    "    return aggregated_shap_interaction_values, aggregated_feature_values, abstract_feature_names\n",
    "\n",
    "# Aggregate SHAP interaction values and feature values\n",
    "aggregated_shap_interaction_values, aggregated_feature_values, abstract_feature_names = aggregate_shap_interaction_values(\n",
    "    shap_interaction_values, X.values, X.columns.tolist(), abstract_features\n",
    ")\n",
    "\n",
    "# Summarize the interaction values\n",
    "tmp = np.abs(aggregated_shap_interaction_values).sum(0)\n",
    "for i in range(tmp.shape[0]):\n",
    "    tmp[i, i] = 0\n",
    "inds = np.argsort(-tmp.sum(0))[:50]\n",
    "tmp2 = tmp[inds, :][:, inds]\n",
    "\n",
    "# Plot the heatmap\n",
    "pl.figure(figsize=(12, 12))\n",
    "pl.imshow(tmp2, cmap='viridis')\n",
    "pl.yticks(range(tmp2.shape[0]), [abstract_feature_names[i] for i in inds], rotation=50.4, horizontalalignment=\"right\")\n",
    "pl.xticks(range(tmp2.shape[0]), [abstract_feature_names[i] for i in inds], rotation=50.4, horizontalalignment=\"left\")\n",
    "pl.gca().xaxis.tick_top()\n",
    "pl.colorbar()\n",
    "pl.title(\"SHAP Interaction Values for Abstract Features\")\n",
    "pl.show()"
   ],
   "id": "e53ef9b95e8563e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ae430e4a9edb4c2a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
